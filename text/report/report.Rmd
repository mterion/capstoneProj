---
title: "Natural Language Processing"
author: "Manu"
date: "14/12/2020"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "C:/Users/cliva/OneDrive - Analytic Base/personalFilesManu/data_science/coursera_courses/data_scientist/10_capstoneProj") # Refer Rmd to the project dir 
```
<!-- Need first: Cover page + Table of content + Abstract https://rpubs.com/EmmanuelClivaz/704581 --> 

## 1. Introduction
### 1.1 Objectives
The objective of the present work is to develop a smart keyboard to enable people to be more effective on their mobile devices. A predictive text model has been developed, giving the user of mobile device three options for what the next word might be.

### 1.2 Context
To develop such a model, a large corpus of text documents was created by merging three different types of english sources: blogs, news and twitts. The raw data, [*The Capstone Data Set*](https://www.coursera.org/lecture/data-science-project/welcome-to-the-capstone-project-uUGxK), was provided by John Hopkins University and the whole code used for creating this report and the predictive model is available on [*Github*](https://github.com/mterion/capstoneProj).

### 1.3 Summary
#### 1.3.1 Research question
The research question addressed is : "How can an efficient text predicitve model be developed for users of mobile devices on the base of publicly available data such as blogs, news wires and tweets ?". This implies that the methodology developed in this work can be replicated for any language, if needed.

#### 1.3.2 Conclusion
#### 1.3.3 Outline of the report

## 2. Sections
### 2.1 Data
#### 2.1.1 Raw data
```{r rawNr, include = FALSE}
nrOfLinesTotal <- format(readRDS("./figures/finalFigures/nrOfLinesTotal.RDS"), big.mark = "'")
nrOfWordsTotal <- format(readRDS("./figures/finalFigures/nrOfWordsTotal.RDS"), big.mark = "'")
```
The data is composed of more that four millions documents, the extact total being `r nrOfLinesTotal`. These documents contain `r nrOfWordsTotal` words. The following table indicates the different statistics related to the three different file sources. The results highlight that some blog documents appear to be very long when compared to the medians of all types of documents.

```{r fileSummaryTable, fig.height= 2.05 }
rm(nrOfLinesTotal, nrOfWordsTotal)
readRDS("./figures/finalFigures/fSumTable.RDS")
```

#### 2.1.2 Data characteristics
A preliminary investigation was conducted to understand data properties, patterns and suggest modelling strategies. The following histograms show the distribution of words across the different file sources.
```{r histWordCount, fig.height = 6.5}
readRDS("./figures/finalFigures/histWordCount.RDS")
```
The distribution of blogs documents is positively skewed (right-skewed) highlighting the fact that a few blogs contain a lot of words. It further indicates the characteristics of a poisson distribution. 

The distribution of news documents appear to be slightly bimodal. Finally, the sharp contrast of both with the distribution of tweets that are much shorter in termes of number of words per document should be noted.

The following violin plots show the full distribution of words across each source. The probability density is shown at different values. The median, interquartiles ranges and other statistics can be consulted by hovering on them. It is interesting to note that blogs have a lower median than news but far more outliers.


```{r violinWordCount}
readRDS("./figures/finalFigures/violinWordCount.RDS")
```

#### 2.1.3 Data pre-processing
Before conducting analysis and developping models, the data needed to be pre-processed. The sequence applied is the following:

* Corpus creation: All documents from the three different sources were merged into a single corpus.
* Tokenization: All texts contained in the corpus were separated into smaller units called tokens which can be words, characters or subwords.
* Token conversion: Token were converted to lower case
* Token cleaning:
    + All Punctuation, symbols such as emoji, url, separators and isolated numbers were removed.
    + Urls removal: a second url filter was applied by considering 1504 [domain names](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)
    + All existing tokens were split on punctuations marks or symbols and the separators removed. (e.g. "2.2" would make two tokens of "2")
    + All tokens consisting of numbers were removed.
    + All profanity words were deleted. The profanity filter built used this [data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)
    + A personal dictionary was built to contain other unsignificant words needing to be discarded (e.g. "mg", "rd"...)
    + All stopwords were removed for exploratory analysis, but a token data set including tokens with stop words was kept for later model development purposes
   
After following this pre-processing sequence, the data contained in the corpus was deemed ready for further analysis and model development.


### 2.2 Methods
### 2.3 Analysis
### 2.3.1 Exploratory analysis 
#### n-grams bar plots {.tabset}
N-grams are extensively used in natural language processing tasks; they are a set of co-occuring words within a given window. For example, if n-grams = 1, all words of the corpus will be listed one after the other. If n-grams = 2, it will take the word and one word forward, aggregate them and move to the next world. 

Here is an example for n-grams = 2, the sentence being "here is an ngram demo": 

* here_is
* is_an
* an_ngram
* ngram_demo

For this research, the values of n-grams = 1, 2, 3 were considered. The following pannel plot display 15 of the most frequent features for each n-grams value and data source.


##### nGrams1
```{r nGram1BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram1.RDS")
```


##### nGrams2
```{r nGram2BarPlot}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram2.RDS")
```

##### nGrams3
```{r nGram3BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram3.RDS")
``` 


#### n-grams tables {.tabset}
To enable an in depth analysis of the data and its structure, the following table provides a detailed review of the 50'000 most frequent features for each source, across the three n-grams values. 

The characteristics of the languages from Twitts where the same words can be repeated a few consecutive time (e.g. Fun, fun, fun!) should be noted. Such characteristics are totally absent in news, where the language is more formal. The same occur with word abbreviations, for example "u" versus "you".


##### n1A
```{r n1A}
        readRDS("./figures/finalFigures/nGram_1_TableAll.RDS")
``` 


##### n1B
```{r n1B}
        readRDS("./figures/finalFigures/nGram_1_TableBlogs.RDS")
``` 

##### n1N
```{r n1N}
        readRDS("./figures/finalFigures/nGram_1_TableNews.RDS")
``` 

##### n1T
```{r n1T}
        readRDS("./figures/finalFigures/nGram_1_TableTwitts.RDS")
``` 

##### n2A
```{r n2A}
        readRDS("./figures/finalFigures/nGram_2_TableAll.RDS")
``` 

##### n2B
```{r n2B}
        readRDS("./figures/finalFigures/nGram_2_TableBlogs.RDS")
``` 

##### n2N
```{r n2N}
        readRDS("./figures/finalFigures/nGram_2_TableNews.RDS")
``` 

##### n2T
```{r n2T}
        readRDS("./figures/finalFigures/nGram_2_TableTwitts.RDS")
``` 

##### n3A
```{r n3A}
        readRDS("./figures/finalFigures/nGram_3_TableAll.RDS")
``` 

##### n3B
```{r n3B}
        readRDS("./figures/finalFigures/nGram_3_TableBlogs.RDS")
``` 

##### n3N
```{r n3N}
        readRDS("./figures/finalFigures/nGram_3_TableNews.RDS")
``` 

##### n3T
```{r n3T}
        readRDS("./figures/finalFigures/nGram_3_TableTwitts.RDS")
``` 


#### n-grams tables with stop words {.tabset}
To understand the particularities of the content of the different sources, stop words (e.g. a, and, but, how, etc.) were removed in the previous tables. Neverthelesse, a predictive model that will help the user predict the next word when using his mobile device requires to integrate stop words in the model prediction. Therefore, the next table provide an overview of the integrated dataset, with the inclusion of stop words.


##### n1ASW
```{r n1ASW}
        readRDS("./figures/finalFigures/nGram_1_TableAllSW.RDS")
``` 

##### n2ASW
```{r n2ASW}
        readRDS("./figures/finalFigures/nGram_2_TableAllSW.RDS")
``` 

##### n3ASW
```{r n3ASW}
        readRDS("./figures/finalFigures/nGram_3_TableAllSW.RDS")
``` 


#### Number of unique words
```{r totalNrWord, include = FALSE}
nrWordC <- format(readRDS("./figures/finalFigures/nrWordC.RDS"), big.mark = "'")
coverage0.5 <- format(readRDS("./figures/finalFigures/coverage0.5.RDS"), big.mark = "'")
coverage0.9 <- format(readRDS("./figures/finalFigures/coverage0.9.RDS"), big.mark = "'")
```
The number of unique words in the corpus is `r nrWordC`. The amount of unique words (including stop words) needed to cover a percentage of all word instances mentioned in this corpus can be seen in the following table. It requires for example `r coverage0.5` unique words to cover 50% of all word instances and `r coverage0.9` unique words to cover 90% of all word instances present in all documents studied.

```{r coverageTable}
rm(nrWordC, coverage0.5, coverage0.9)
readRDS("./figures/finalFigures/coverageTable.RDS")
```

### 2.4 Results
The different tasks (1-2) answers are the following:

1. **How do you evaluate how many of the words come from foreign languages?** 
This can be done by using a foreign language dictionary and filtering out the words that belongs to it. It goes in the same direction than the work done with the removal of profanity words on the base of a special dictionary.

2. **Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?**
     i) Using a smaller number of words in the dictionary to cover the same number of phrase can simply be done by removing the dictionary low-frequency words. It can also be done by reducing the number of low frequency words by stemming.
     i) Increasing the coverage can be done by comparing the corpus dictionary with a full language dictionary, taking the delta and thinking about why this words were not in our original corpus. This will give indication at whether we should include other types of data sources (blogs of different types, etc.) in our actual corpus.
     
     
3. **How can you efficiently store an n-gram model (think Markov Chains)?** The efficient storage of a markov chain model is that it stores the probability in a sequence of events based on the state attained in the previous event. Thus the clear advantage it promote efficiency in the storage of the model. It suits perfectly for sequence processing as done in natural language processing.
     
4. **How can you use the knowledge about word frequencies to make your model smaller and more efficient?**


5. **How many parameters do you need (i.e. how big is n in your n-gram model)?**

6. **Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?**

7. **How do you evaluate whether your model is any good?**

8. **How can you use backoff models to estimate the probability of unobserved n-grams?**

 


## 3. Conclusion
### 3.1 Research question addressed
### 3.2 Results obtained
### 3.3 Recommendations

## 4. Appendix
### 4.1 Details of data and process
### 4.2 References
[Profanity list](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)
[Data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)





