---
title: "Natural Language Processing"
author: "Manu"
date: "14/12/2020"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "C:/Users/cliva/OneDrive - Analytic Base/personalFilesManu/data_science/coursera_courses/data_scientist/10_capstoneProj") # Refer Rmd to the project dir 
```
<!-- Need first: Cover page + Table of content + Abstract https://rpubs.com/EmmanuelClivaz/704581 --> 

## 1. Introduction
### 1.1 Objectives
The objective of the present work is to develop a smart keyboard to enable people to be more effective on their mobile devices. A predictive text model has been developed, giving the user of mobile device three options for what the next word might be.

### 1.2 Context
To develop such a model, a large corpus of text documents was created by merging three different types of english sources: blogs, news and twitts. The raw data, [*The Capstone Data Set*](https://www.coursera.org/lecture/data-science-project/welcome-to-the-capstone-project-uUGxK), was provided by John Hopkins University and the whole code used for creating this report and the predictive model is available on [*Github*](https://github.com/mterion/capstoneProj).

### 1.3 Summary
#### 1.3.1 Research question
The research question addressed is : "How can an efficient text predicitve model be developed for users of mobile devices on the base of publicly available data such as blogs, news wires and tweets ?". This implies that the methodology developed in this work can be replicated for any language, if needed.

#### 1.3.2 Conclusion
#### 1.3.3 Outline of the report

## 2. Sections
### 2.1 Data
#### 2.1.1 Raw data
The data is composed of more that four millions documents, the extact total being 4'269'678. The following table indicates the different statistics related to the three different file sources. The results highlight that some blog documents appear to be very long when compared to the medians of all types of documents.
```{r fileSummaryTable, fig.height= 2.05 }
readRDS("./figures/finalFigures/fSumTable.RDS")
```

#### 2.1.2 Data characteristics
A preliminary investigation was conducted to understand data properties, patterns and suggest modelling strategies. The following histograms show the distribution of words across the different file sources.
```{r histWordCount, fig.height = 6.5}
readRDS("./figures/finalFigures/histWordCount.RDS")
```
The distribution of blogs documents is positively skewed (right-skewed) highlighting the fact that a few blogs contain a lot of words. It further indicates the characteristics of a poisson distribution. 

The distribution of news documents appear to be slightly bimodal. Finally, the sharp contrast of both with the distribution of tweets that are much shorter in termes of number of words per document should be noted.

The following violin plots show the full distribution of words across each source. The probability density is shown at different values. The median, interquartiles ranges and other statistics can be consulted by hovering on them. It is interesting to note that blogs have a lower median than news but far more outliers.


```{r violinWordCount}
readRDS("./figures/finalFigures/violinWordCount.RDS")
```

#### 2.1.3 Data pre-processing
Before conducting analysis and developping models, the data needed to be pre-processed. The sequence applied is the following:

* Non-wanted character removals: All non-essential characters were removed with regular expressions to facilitate analysis and model development (e.g.: ">,<, =, ~, #"). The numbers separated by "-" were also deleted.
* Corpus creation: All documents from the three different sources were merged into a single corpus.
* Tokenization: All texts contained in the corpus were separated into smaller units called tokens which can be words, characters or subwords.
* Token conversion: Token were converted to lower case
* Token removal
    + Punctuation, symbols such as emoji, url, separators and isolated numbers were removed.
    + All profanity words were deleted. The profanity filter built used this [data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)
    + All stopwords were removed. 
    + A personal dictionary was built to contain other unsignificant words needing to be discarded
    
   
After following this pre-processing sequence, the data contained in the corpus were deemed ready for further analysis and model development.


### 2.2 Methods
### 2.3 Analysis
### 2.3.1 Exploratory analysis 
#### n-grams bar plots {.tabset}
N-grams are extensively used in natural language processing tasks; they are a set of co-occuring words within a given window. For example, if N = 1, all words of the corpus will be listed one after the other. If N = 2, it will take the word and one word forward, aggregate them and move to the next world. 

Here is an example for N = 2, the sentence being "here is an ngram demo": 

* here_is
* is_an
* an_ngram
* ngram_demo

For this research, the values of N = 1, N= 2, N = 3 were considered. A pannel plot of the 15 most frequent features for each source is displayed underneath.


##### nGrams1
```{r nGram1BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram1.RDS")
```


##### nGrams2
```{r nGram2BarPlot}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram2.RDS")
```

##### nGrams3
```{r nGram3BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram3.RDS")
``` 


#### n-grams tables {.tabset}
To facilitate an in depth analysis of the data and its structure, the following table provides a detailed review of the 50'000 most frequent features for each source, across the three N-grams values.

##### n1A
```{r n1A}
        readRDS("./figures/finalFigures/nGram_1_TableAll.RDS")
``` 


##### n1B
```{r n1B}
        readRDS("./figures/finalFigures/nGram_1_TableBlogs.RDS")
``` 

##### n1N
```{r n1N}
        readRDS("./figures/finalFigures/nGram_1_TableNews.RDS")
``` 

##### n1T
```{r n1T}
        readRDS("./figures/finalFigures/nGram_1_TableTwitts.RDS")
``` 

##### n2A
```{r n2A}
        readRDS("./figures/finalFigures/nGram_2_TableAll.RDS")
``` 

##### n2B
```{r n2B}
        readRDS("./figures/finalFigures/nGram_2_TableBlogs.RDS")
``` 

##### n2N
```{r n2N}
        readRDS("./figures/finalFigures/nGram_2_TableNews.RDS")
``` 

##### n2T
```{r n2T}
        readRDS("./figures/finalFigures/nGram_2_TableTwitts.RDS")
``` 

##### n3A
```{r n3A}
        readRDS("./figures/finalFigures/nGram_3_TableAll.RDS")
``` 

##### n3B
```{r n3B}
        readRDS("./figures/finalFigures/nGram_3_TableBlogs.RDS")
``` 

##### n3N
```{r n3N}
        readRDS("./figures/finalFigures/nGram_3_TableNews.RDS")
``` 

##### n3T
```{r n3T}
        readRDS("./figures/finalFigures/nGram_3_TableTwitts.RDS")
``` 


#### Number of unique words 
The number of unique words needed to cover a certain percentage of all word instances in the language considered can be seen in the following table. It requires for example xxx unique words to cover 90% of all word instances in the documents studied.





### 2.4 Results


 * Corpus size
 * Balance, representativeness and sampling
 


## 3. Conclusion
### 3.1 Research question addressed
### 3.2 Results obtained
### 3.3 Recommendations

## 4. Appendix
### 4.1 Details of data and process
### 4.2 References
[data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)





