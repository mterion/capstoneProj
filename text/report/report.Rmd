---
title: "Natural Language Processing"
author: "Manu"
date: "26/12/2020"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "C:/Users/cliva/OneDrive - Analytic Base/personalFilesManu/data_science/coursera_courses/data_scientist/10_capstoneProj") # Refer Rmd to the project dir 
```
<!-- Need first: Cover page + Table of content + Abstract https://rpubs.com/EmmanuelClivaz/704581 --> 

## 1. Introduction
### 1.1 Objectives
The objective of the present work is to develop a smart keyboard to enable people to be more effective on their mobile devices. A predictive text model has been developed, giving the user of mobile device three options for what the next word might be.

### 1.2 Context
To develop such a model, a large corpus of text documents was created by merging three different types of english sources: blogs, news and twitts. The raw data, [The Capstone Data Set](https://www.coursera.org/lecture/data-science-project/welcome-to-the-capstone-project-uUGxK), was provided by John Hopkins University and the whole code used for creating this report and the predictive model is available on [Github](https://github.com/mterion/capstoneProj).

### 1.3 Summary
#### 1.3.1 Research question
The research question addressed is : "How can an efficient text predicitve model be developed for users of mobile devices on the base of publicly available data such as blogs, news wires and tweets ?". This implies that the methodology developed in this work can be replicated for any language, if needed.
<!-- 
#### 1.3.2 Conclusion
#### 1.3.3 Outline of the report
-->

## 2. Sections
### 2.1 Data
#### 2.1.1 Raw data
```{r rawNr, include = FALSE}
nrOfLinesTotal <- format(readRDS("./figures/finalFigures/nrOfLinesTotal.RDS"), big.mark = "'")
nrOfWordsTotal <- format(readRDS("./figures/finalFigures/nrOfWordsTotal.RDS"), big.mark = "'")
```
The data is composed of more that four millions documents, the extact total being `r nrOfLinesTotal`. These documents contain `r nrOfWordsTotal` words. The following table indicates the different statistics related to the three different file sources. The results highlight that some blog documents appear to be very long when compared to the medians of all types of documents.

```{r fileSummaryTable, fig.height= 2.05 }
rm(nrOfLinesTotal, nrOfWordsTotal)
readRDS("./figures/finalFigures/fSumTable.RDS")
```

#### 2.1.2 Data characteristics
A preliminary investigation was conducted to understand data properties, patterns and suggest modelling strategies. The following histograms show the distribution of words across the different file sources.
```{r histWordCount, fig.height = 6.5}
readRDS("./figures/finalFigures/histWordCount.RDS")
```
The distribution of blogs documents is positively skewed (right-skewed) highlighting the fact that a few blogs contain a lot of words. It further indicates the characteristics of a poisson distribution. 

The distribution of news documents appear to be slightly bimodal. Finally, the sharp contrast of both with the distribution of tweets that are much shorter in termes of number of words per document should be noted.

The following violin plots show the full distribution of words across each source. The probability density is shown at different values. The median, interquartiles ranges and other statistics can be consulted by hovering on them. It is interesting to note that blogs have a lower median than news but far more outliers.


```{r violinWordCount}
readRDS("./figures/finalFigures/violinWordCount.RDS")
```

#### 2.1.3 Data pre-processing
Before conducting analysis and developping models, the data needed to be pre-processed. The sequence applied is the following:

* Corpus creation: All documents from the three different sources were merged into a single corpus.
* Removal of [twitter abbreviations](https://www.socialmediatoday.com/content/top-twitter-abbreviations-you-need-know) before tokenization.
* Tokenization: All texts contained in the corpus were separated into smaller units called tokens which can be words, characters or subwords.
* Token conversion: Token were converted to lower case
* Token cleaning:
    + All Punctuation, symbols such as emoji, url, separators and isolated numbers were removed.
    + Urls removal: a second url filter was applied by considering 1504 [domain names](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)
    + All tokens consisting of numbers were removed.
    + All profanity words were deleted. The profanity filter built used this [data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)
    + A personal dictionary was built to contain other unsignificant words needing to be discarded (e.g. "mg", "rd"...)
    + All stopwords were removed for exploratory analysis, but a token data set including tokens with stop words was kept for later model development purposes
   
After following this pre-processing sequence, the data contained in the corpus was deemed ready for further analysis and model development.


### 2.2 Methods
### 2.3 Analysis
### 2.3.1 Exploratory analysis 
#### n-grams bar plots {.tabset}
N-grams are extensively used in natural language processing tasks; they are a set of co-occuring words within a given window. For example, if n-grams = 1, all words of the corpus will be listed one after the other. If n-grams = 2, it will take the word and one word forward, aggregate them and move to the next world. 

Here is an example for n-grams = 2, the sentence being "here is an ngram demo": 

* here_is
* is_an
* an_ngram
* ngram_demo

For this research, the values of n-grams = 1, 2, 3 were considered. The following pannel plot display 15 of the most frequent features for each n-grams value and data source.


##### nGrams1
```{r nGram1BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram1.RDS")
```


##### nGrams2
```{r nGram2BarPlot}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram2.RDS")
```

##### nGrams3
```{r nGram3BarPlot, fig.height= 6}
      readRDS("./figures/finalFigures/pannelFeatPlotNGram3.RDS")
``` 


#### n-grams tables {.tabset}
To enable an in depth analysis of the data and its structure, the following table provides a detailed review of the 50'000 most frequent features for each source, across the three n-grams values. 

The characteristics of the languages from Twitts where the same words can be repeated a few consecutive time (e.g. Fun, fun, fun!) should be noted. Such characteristics are totally absent in news, where the language is more formal. The same occur with word abbreviations, for example "u" versus "you".


##### n1A
```{r n1A}
        readRDS("./figures/finalFigures/nGram_1_TableAll.RDS")
``` 

##### n1B
```{r n1B}
        readRDS("./figures/finalFigures/nGram_1_TableBlogs.RDS")
``` 

##### n1N
```{r n1N}
        readRDS("./figures/finalFigures/nGram_1_TableNews.RDS")
``` 

##### n1T
```{r n1T}
        readRDS("./figures/finalFigures/nGram_1_TableTwitts.RDS")
``` 

##### n2A
```{r n2A}
        readRDS("./figures/finalFigures/nGram_2_TableAll.RDS")
``` 

##### n2B
```{r n2B}
        readRDS("./figures/finalFigures/nGram_2_TableBlogs.RDS")
``` 

##### n2N
```{r n2N}
        readRDS("./figures/finalFigures/nGram_2_TableNews.RDS")
``` 

##### n2T
```{r n2T}
        readRDS("./figures/finalFigures/nGram_2_TableTwitts.RDS")
``` 

##### n3A
```{r n3A}
        readRDS("./figures/finalFigures/nGram_3_TableAll.RDS")
``` 

##### n3B
```{r n3B}
        readRDS("./figures/finalFigures/nGram_3_TableBlogs.RDS")
``` 

##### n3N
```{r n3N}
        readRDS("./figures/finalFigures/nGram_3_TableNews.RDS")
``` 

##### n3T
```{r n3T}
        readRDS("./figures/finalFigures/nGram_3_TableTwitts.RDS")
``` 


#### Semantic network analys {.tabset}
Semantic relations between words (n-grams1) are displayed underneath. A feature co-occurence matrix was constructed in order to analyze the patterns of the 50 most frequent words from each source. The intensity of the relations between words shows interesting patterns. 

Twitts, beside highlighting a specific abbreviated type of language, show stronger co-occurence of words related to emotions (great, time, thanks, happy...) and a time frame limited to activities carried out now (day, morning, weekend, time, today...). On the other end of the spectrum, and not surprisingly, news are detached from emotions:  Co-occurences of words are factual and the time frame window is much larger. Finally, blogs appear to in a category between those two extremes, having more factual content than twitts but still being emotionally related.

##### sNetwAll
```{r sNetwAll}
        readRDS("./figures/finalFigures/semNetwCAll.RDS")
``` 

##### sNetwBlogs
```{r sNetBlogs}
        readRDS("./figures/finalFigures/semNetwCBlogs.RDS")
```

##### sNetwNews
```{r sNetwNews}
        readRDS("./figures/finalFigures/semNetwCNews.RDS")
```

##### sNetwTwitts
```{r sNetwTwitts}
        readRDS("./figures/finalFigures/semNetwCTwitts.RDS")
``` 


#### n-grams tables with stop words {.tabset}
To understand the particularities of the content of the different sources, stop words (e.g. a, and, but, how, etc.) were removed in the previous tables and analyses. Nevertheless, a predictive model that should help the user predict the next word when using his mobile device, requires to integrate stop words in model prediction. Therefore, the next table provide an overview of the integrated dataset, with the inclusion of stop words.


##### n1ASW
```{r n1ASW}
        readRDS("./figures/finalFigures/nGram_1_TableAllSW.RDS")
``` 

##### n2ASW
```{r n2ASW}
        readRDS("./figures/finalFigures/nGram_2_TableAllSW.RDS")
``` 

##### n3ASW
```{r n3ASW}
        readRDS("./figures/finalFigures/nGram_3_TableAllSW.RDS")
``` 


#### Number of unique words
```{r totalNrWord, include = FALSE}
nrWordC <- format(readRDS("./figures/finalFigures/nrWordC.RDS"), big.mark = "'")
coverage0.5 <- format(readRDS("./figures/finalFigures/coverage0.5.RDS"), big.mark = "'")
coverage0.9 <- format(readRDS("./figures/finalFigures/coverage0.9.RDS"), big.mark = "'")
```
The number of unique words in the corpus is `r nrWordC`. The amount of unique words (including stop words) needed to cover a percentage of all word instances mentioned in this corpus can be seen in the following table. It requires for example `r coverage0.5` unique words to cover 50% of all word instances and `r coverage0.9` unique words to cover 90% of all word instances present in all documents studied.

```{r coverageTable, fig.height=1.2}
rm(nrWordC, coverage0.5, coverage0.9)
readRDS("./figures/finalFigures/coverageTable.RDS")
```

### 2.3.2 Models{.tabset}

#### Summary 
A statistical language model is a distribution of probabilities over a sequence of word: $P(w_{1},\ w_{2},\ ...,\ w_{m})$.^[https://en.wikipedia.org/wiki/Language_model] 
In such a language, n-gram are made by considering sequences of words. For example a ngram2 or bigram has the sequence $(w_{i-1},\ w_{i})$ and an ngram3 or trigram the sequence: $(w_{i-2},\ w_{i-1},\ w_{i})$.

#### SBO
A simple model is the SBO which does not generate probabilities or apply discount, but use relative frequencies interpreted as a score "S". A SBO model is relatively inexpensive to train and works well on large corpora^[https://www.aclweb.org/anthology/D07-1090.pdf] 

The model start with ngram3 and backoff to ngram2 and finally ngram1 if no search hits are returned. If there are hits within ngrams, the word with the highest frequency is returned. For example, if the user input is "I want to go" : 

* A search is first conducted in the ngram3 $(w_{i-2},\ w_{i-1},\ w_{i})$. The following ngram3 could have been, for example, observed: "to_go_home", "to_go_swimming", "...". 

* If none are observed, it will back-off to a search in the ngram2 $(w_{i-1},\ w_{i})$. The following ngram2 could have been observed: "go_far", "go_west", "...".

* If none are observed, then the final search is conducted in the ngram1 $(w_{i})$. The following ngram1 could have been observed: "you", "with", "...".

It should be noted that the ngram2 and ngram1 procedures have a backoff factor alpha that is applied to the selected frequencies. Please refer to the equations underneath for more details.


##### SBO estimate
The SBO estimate are the following:

* **ngram3**:
$S_{SBO}(w_{i}|w_{i-2}, w_{i-1})\ =$
    + if ${C(w_{i-2},\ w_{i-1},\ w_{i}) >0 }$ then $\frac{C(w_{i-2},\ w_{i-1},\ w_{i})}{C(w_{i-2},\ w_{i-1})}$
    + else $\alpha\ * S_{SBO}(w_{i}|w_{i-1})$ which is the following algorithm for ngram2  
<p>&nbsp;</p>

* **ngram2**:
$S_{SBO}(w_{i}|w_{i-1})\ =$
     + if $C(w_{i-1},\ w_{i}) > 0$ then $\frac{C(w_{i-1}, w_{i})}{C(w_{i-1})}$
     + else $\alpha\ * \frac {C(w_{i})}{\sum_{j=1}^{m}C(w_{j})}$ which is simply the ML of ngram1

The backoff factor $\alpha$ with a value of 0.4 was used based on experiments made by the authors of this model.


#### KBO
##### Probabilities estimation
The Katz's back-off model consider the conditional probability of a word given the immediate n-gram preceding sequence.^[https://en.wikipedia.org/wiki/Katz's_back-off_model#cite_note-2] The strategy is to use higher order n-gram model if enough data is available and to back off to lower order n-gram when data is missing. Given the fact that many words may not appear in the training data, discounting need to be used to avoid zero probabilities issues.^[Manning & Schütze, 1999, Foundation of statistical natural language processing, MIT, p.255]

Discounting methods allow to somewhat decrease the probability of seen events, in order to obtain some probability mass that can be assigned to unseen events. The process of discounting is referred as smoothing, due to the removal of zeroes. ^[Ibid, p.252]

Due to the size of the corpus, the computing time of the algorithm needed to be considered. Therefore, if a search in ngram3 was successful and amounted to more than 5 occurrences ($k>5$) of the same ngram3, the conditional probability of a word given its history is proportional to its maximum likelihood estimate: 

$EstP(w_{i}|w_{i-2},\ w_{i-1}) =$

* if $C(w_{i-2},\ w_{i-1},\ w_{i}) > 5$ 
    + then $P_{ML}(w_{i}|w_{i-1}, w_{i-2}) = \frac{C( w_{i-2},\ w_{i-1},\ w_{i})}{C(w_{i-1},\ w_{i})}$

Otherwise, the KBO model is applied.

$Pbo(w_{i}|w_{i\ n+1},\ _{\dots}\ , w_{i}) =$

* if $C(w_{i-n+1},\ _{\dots}\ , w_{i}) > 0$ 
    + then $d_{w_{i-n+1,\ ...,\ w_{i}}} \ \frac{C(w_{i-n+1},\ _{\dots}\ ,\ w_{i-1},\ w_{i})}{C(w_{i-n+1},\ _{\dots}\ ,\ w_{i-1}}$
* else $\alpha_{wi-n+1,\ ...,\ w_{i-1}} Pbo(w_{i}|w_{i-n+2},\ _{\dots}\ ,w_{i-1})$


Where $d$ is the amount of discount applied with the typical value of 0.5 chosen.^[http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf] The definitions of the discounted counts for both ngrams are: 

* For ngram2: $c^*(w_{i-1},\ w_{i})=c(w_{i-1},\ w_{i}) - d$
* For ngram3: $c^*(w_{i-2},\ w_{i-1},\ w_{i})=c(w_{i-2},\ w_{i-1},\ w_{i}) - d$

The backed off probabilities after discount for observed ngrams are then:

* For ngram2: $\frac{c^*(w_{i-1},\ w_{i})}{c(w_{i-1})}$
* For ngram3: $\frac{c^*(w_{i-2},\ w_{i-1},\ w_{i})}{c(w_{i-2},\ w_{i-1})}$

This leads to $\alpha$ which is the missing probability mass taken from observed ngrams defined as:

* For ngram2: $\alpha(w_{i-1}) = 1-\sum_{w\in A_{(w_{i-1})}} \frac{c^*(w_{i-1},\ w_{i})}{c(w_{i})}$
* For ngram3: $\alpha(w_{i-2},\ w_{i-1}) = 1-\sum_{w\in A_{(w_{i-2},\ w_{i-1})}} \frac{c^*(w_{i-2},\ w_{i-1},\ w_{i})}{c(w_{i-1},\ w_{i})}$
    + Where $A$ means all the words terminating an ngram.^[http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf]

Finally, the missing probability mass is assigned to unobserved ngram :

* For ngram2: $\alpha(w_{i-1}) = \frac{q_{ML}(w_{i})}{\sum_{w\in B_{(w_{i-1})}} q_{ML}(w)}$ 
* For ngram3: $\alpha(w_{i-2},\ w_{i-1}) = \frac{q_{BO}(w_{i}|w_{i-1})}{\sum_{w\in B_{(w_{i-2},\ w_{i-1})}} q_{BO}(w|w_{i-1})}$
    + Where $B$ means all the remaining words in the vocabulary.

In the very unlikely case that both words constituting the last words of the user input are both unknown in the whole vocabulary of the Corpus, the $P_{ML}$ of ngram1 are returned.


### 2.4 Results




The different tasks (1-2) answers are the following:


 

<!--
## 3. Conclusion
### 3.1 Research question addressed
### 3.2 Results obtained
### 3.3 Recommendations

## 4. Appendix
### 4.1 Details of data and process
### 4.2 References
[Twitter abbreviations](https://www.socialmediatoday.com/content/top-twitter-abbreviations-you-need-know)

[Profanity list](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)

[Data source](https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en)

[N-gram language models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)

-->





